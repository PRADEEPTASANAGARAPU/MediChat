{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXMVoEFkPXJJ"
      },
      "source": [
        "### **Install Ludwig and Ludwig's LLM related dependencies.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfYXyUUvM-R6"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow --quiet\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers\n",
        "!pip install --upgrade git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/ludwig-ai/ludwig.git@master --quiet\n",
        "\n",
        "# !pip show torch\n",
        "# !pip show transformers\n",
        "\n",
        "!pip install --upgrade datasets\n",
        "!pip install py7zr\n",
        "!pip install xformers\n",
        "!pip install accelerate\n",
        "# !pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install dask[dataframe]\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "!pip uninstall -y torch torchvision torchaudio torchtext --quiet\n",
        "!pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --force-reinstall scipy pandas matplotlib"
      ],
      "metadata": {
        "id": "LzkXyaLnEh_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd_LA_2Wx_qr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import gc\n",
        "from typing import Any, Callable\n",
        "import time\n",
        "from functools import wraps\n",
        "from inspect import ( BoundArguments, signature )\n",
        "from collections import OrderedDict\n",
        "from google.colab import data_table\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import datasets\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, LlamaForCausalLM, MistralForCausalLM, AutoTokenizer, LlamaTokenizerFast, GenerationConfig, TextGenerationPipeline, BatchEncoding\n",
        "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
        "from peft import PeftModel, PeftModelForCausalLM, PeftConfig, LoraConfig\n",
        "from ludwig.api import LudwigModel, TrainingResults\n",
        "import logging\n",
        "\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from google.colab import drive\n",
        "\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2SRHcKAJYuu"
      },
      "source": [
        "Enable text wrapping so we don't have to scroll horizontally and create a function to flush CUDA cache."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "SLcKgqPg6qc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "DATASET_NAME = \"FreedomIntelligence/medical-o1-reasoning-SFT\"\n",
        "CONFIG_NAME = \"en\"  # 'en', 'zh', 'en_mix', or 'zh_mix'\n",
        "OUTPUT_FILE = f\"medical{CONFIG_NAME}.json\"\n",
        "\n",
        "# Load the dataset (no split)\n",
        "print(f\"Loading dataset: {DATASET_NAME} with config: {CONFIG_NAME}\")\n",
        "dataset = load_dataset(DATASET_NAME, CONFIG_NAME)\n",
        "\n",
        "# Save to JSON â€” Hugging Face loads splits as a dict\n",
        "# So we save each split (usually only 'train') separately\n",
        "for split_name, split_data in dataset.items():\n",
        "    split_output_file = f\"{OUTPUT_FILE.rsplit('.', 1)[0]}_{split_name}.json\"\n",
        "    split_data.to_json(split_output_file)\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "8NjbWwkOAbOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import csv\n",
        "from datasets import load_dataset\n",
        "\n",
        "DELAY_SECONDS = 1\n",
        "for split_name, split in dataset.items():\n",
        "    with open(OUTPUT_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        headers = split.column_names\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for example in split:\n",
        "            writer.writerow([example.get(col, \"\") for col in headers])\n",
        "            time.sleep(0)  # Delay between rows\n",
        "\n",
        "print(f\"Extraction complete! Data saved to {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "id": "O-RZCrhp_Ue8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update Dataset to MongoDB\n",
        "\n",
        "- generate the dataset in the form of questionarie using \"question-generation\" model.\n",
        "- Pushing the current Abstracts to MongoDB to re-use the dataset for persistant storage option.\n",
        "- increase the dataset size by collecting weekly to get new abstracts"
      ],
      "metadata": {
        "id": "gqV8lIcWA2UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = pymongo.MongoClient(\"mongodb+srv://demo:demo@cluster.mongodb.net\")\n",
        "\n",
        "db = client[\"dataset_collection\"]\n",
        "collection = db[\"data\"]\n",
        "\n",
        "def generate_qa_pairs(abstract):\n",
        "    nlp = transformers.pipeline(\"question-generation\")\n",
        "    qa_pairs = nlp(abstract)\n",
        "    return \"input: \"+qa_pairs[\"question\"] + '\\n' + \"answer: \"+qa_pairs[\"answer\"]\n",
        "\n",
        "with open(OUTPUT_FILE, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        train = row[0]\n",
        "        Complex_CoT = row[1]\n",
        "        question = row[2]\n",
        "        respnse = row[3]\n",
        "        document = {\n",
        "            \"train\": train,\n",
        "            \"Complex_CoT\": Complex_CoT,\n",
        "            \"question\": question,\n",
        "            \"response\": respnse,\n",
        "            \"input\": generate_qa_pairs(abstract)\n",
        "        }\n",
        "        collection.insert_one(document)"
      ],
      "metadata": {
        "id": "_FsvuuElBCZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_dFOQDtqY3R"
      },
      "outputs": [],
      "source": [
        "def predict(model: LudwigModel, df_test: pd.DataFrame) -> list[list[str]]:\n",
        "    return model.predict(df_test)[0][\"answer_response\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV0D2rjRbk4z"
      },
      "source": [
        "### **Import Dataset** ðŸ“‹\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TVKCCU4xKuo"
      },
      "source": [
        "#### Import Dataset from MongoDB and connect to Google Drive\n",
        "\n",
        "- The current Dataset will be act as training dataset for the model that's get generated and stored in google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLHPamZCxAt2"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rShxBWx3xKZi"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/MyDrive/project-medichat/'\n",
        "content_path = '/mnt/medical.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "cursor = collection.find()\n",
        "data = []\n",
        "for doc in cursor:\n",
        "    print(doc)\n",
        "    doc[\"train\"] = str(doc[\"train\"])\n",
        "    data.append(doc)\n",
        "\n",
        "with open(content_path, 'w') as file:\n",
        "    json.dump(data, file, indent=4)"
      ],
      "metadata": {
        "id": "wZ5-vWwECyLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVPj_pD8KWlO"
      },
      "source": [
        "#### Preparing Dataset\n",
        "\n",
        "train, test, validation, evaluation\n",
        "\n",
        "- Observations:\n",
        "\n",
        "     1. The maximum no.of train dataset which can be accompanined by the google colab pro is 25k records of abstracts based on model trainer configurations(effective_batch_size, epoch, train_steps).\n",
        "     2. The model is out of memory >25K records where the CUDA GPU device not able to fit the memory during train process and required >40GB memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Path to your file\n",
        "content_path = '/mnt/medical.json'\n",
        "\n",
        "# List to hold all records\n",
        "alldata = []\n",
        "\n",
        "with open(content_path, 'r', encoding='utf-8') as file:\n",
        "    # Read each line and parse it as JSON\n",
        "    for line in file:\n",
        "        alldata.append(json.loads(line.strip()))  # Strip any extra whitespace/newlines\n",
        "\n",
        "print(\"# Total alldata samples:\", len(alldata))"
      ],
      "metadata": {
        "id": "kWx4Sy5TPFEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khAcJ-O8LlE"
      },
      "outputs": [],
      "source": [
        "total_samples = len(alldata)\n",
        "train_split = 0.7  # 70% training\n",
        "val_split = 0.15   # 15% validation\n",
        "test_split = 0.15  # 15% testing\n",
        "\n",
        "# Calculate indices\n",
        "train_end = int(train_split * total_samples)\n",
        "val_end = train_end + int(val_split * total_samples)\n",
        "\n",
        "# Split the data\n",
        "train_dataset = alldata[:train_end]\n",
        "validation_dataset = alldata[train_end:val_end]\n",
        "test_dataset = alldata[val_end:]\n",
        "\n",
        "# Print the sizes\n",
        "print(\"# train_dataset samples:\", len(train_dataset))\n",
        "print(\"# validation_dataset samples:\", len(validation_dataset))\n",
        "print(\"# test_dataset samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "current_directory = Path.cwd()\n",
        "print(current_directory)"
      ],
      "metadata": {
        "id": "nHtk4NiKLyRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ltMKWpAKLf1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NN2vFcj-TJB"
      },
      "outputs": [],
      "source": [
        "# Convert lists to dataframes\n",
        "df_train = pd.DataFrame(train_dataset)\n",
        "df_test = pd.DataFrame(test_dataset)\n",
        "df_validation = pd.DataFrame(validation_dataset)\n",
        "df_evaluation = pd.DataFrame(test_dataset)  # testset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGPq00KmK2f3"
      },
      "source": [
        "### combining dataset to train, test, validation as df_dataset\n",
        "\n",
        "As the initial process, we train the Mistral-7B base model with complete dataset of abstracts. Then we finetune the model with individual train & validation datasets for the Questionarie use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkj-qB7pX_27"
      },
      "outputs": [],
      "source": [
        "from google.colab import data_table;\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc4i-cqFYCFQ"
      },
      "outputs": [],
      "source": [
        "# adding split column to train, test and validation\n",
        "df_train[\"split\"] = np.zeros(df_train.shape[0])\n",
        "df_test[\"split\"] = np.ones(df_test.shape[0])\n",
        "df_validation[\"split\"] = np.full(df_validation.shape[0], 2)\n",
        "\n",
        "# creating a dataset dataframe\n",
        "df_dataset = pd.concat([df_train, df_test, df_validation])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation.head()"
      ],
      "metadata": {
        "id": "2OT7wu6dQyfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset['context'] = df_dataset['train'].apply(lambda x: x['Complex_CoT'])  # 'context' comes from 'Complex_CoT'\n",
        "df_dataset['question'] = df_dataset['train'].apply(lambda x: x['Question'])  # 'question' comes from 'Question'\n",
        "df_dataset['answer'] = df_dataset['train'].apply(lambda x: x['Response'])  # 'answer' comes from 'Response'\n",
        "df_dataset['input'] = 'question: \"' + df_dataset['question'] + '\" \\n context: \"' + df_dataset['context'] + '\"'\n",
        "\n",
        "df_evaluation['context'] = df_evaluation['train'].apply(lambda x: x['Complex_CoT'])  # 'context' comes from 'Complex_CoT'\n",
        "df_evaluation['question'] = df_evaluation['train'].apply(lambda x: x['Question'])  # 'question' comes from 'Question'\n",
        "df_evaluation['answer'] = df_evaluation['train'].apply(lambda x: x['Response'])  # 'answer' comes from 'Response'\n",
        "df_evaluation['input'] = 'question: \"' + df_evaluation['question'] + '\" \\n context: \"' + df_evaluation['context'] + '\"'"
      ],
      "metadata": {
        "id": "h6OY3ntZY-4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J65DIwHDYGGh"
      },
      "outputs": [],
      "source": [
        "df_dataset[\"split\"] = df_dataset[\"split\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = len(df_dataset)\n",
        "train_split = 0.7  # 70% training\n",
        "val_split = 0.15   # 15% validation\n",
        "test_split = 0.15  # 15% testing\n",
        "\n",
        "# Calculate indices\n",
        "train_end = int(train_split * total_samples)\n",
        "val_end = train_end + int(val_split * total_samples)\n",
        "\n",
        "# Split the data\n",
        "train_dataset = df_dataset[:train_end]\n",
        "validation_dataset = df_dataset[train_end:val_end]\n",
        "test_dataset = df_dataset[val_end:]\n",
        "\n",
        "# Print the sizes\n",
        "print(\"# train_dataset samples:\", len(train_dataset))\n",
        "print(\"# validation_dataset samples:\", len(validation_dataset))\n",
        "print(\"# test_dataset samples:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "dGQeinL8cgA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to dataframes\n",
        "df_train = pd.DataFrame(train_dataset)\n",
        "df_test = pd.DataFrame(test_dataset)\n",
        "df_validation = pd.DataFrame(validation_dataset)\n",
        "df_evaluation = pd.DataFrame(test_dataset)  # testset"
      ],
      "metadata": {
        "id": "6wTqbXLscna6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding split column to train, test and validation\n",
        "df_train[\"split\"] = np.zeros(df_train.shape[0])\n",
        "df_test[\"split\"] = np.ones(df_test.shape[0])\n",
        "df_validation[\"split\"] = np.full(df_validation.shape[0], 2)\n",
        "\n",
        "# creating a dataset dataframe\n",
        "df_dataset = pd.concat([df_train, df_test, df_validation])"
      ],
      "metadata": {
        "id": "HVYKsxQhddw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn5yjpbhTzLE"
      },
      "source": [
        "### Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IquAmG9vYSTI"
      },
      "outputs": [],
      "source": [
        "df_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElQzS13HOh0v"
      },
      "outputs": [],
      "source": [
        "df_train.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T737NXc_Own7"
      },
      "outputs": [],
      "source": [
        "df_test.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wwDDFyiO5w4"
      },
      "outputs": [],
      "source": [
        "df_validation.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoJUi1YOYVew"
      },
      "outputs": [],
      "source": [
        "df_dataset.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3x3stdDKoCT"
      },
      "outputs": [],
      "source": [
        "# Calculating the length of each cell in each column\n",
        "df_dataset['num_characters_context'] = df_dataset['context'].apply(lambda x: len(x))\n",
        "df_dataset['num_characters_question'] = df_dataset['question'].apply(lambda x: len(x))\n",
        "df_dataset['num_characters_answer'] = df_dataset['answer'].apply(lambda x: len(x))\n",
        "\n",
        "# Show Distribution\n",
        "df_dataset.hist(column=['num_characters_context', 'num_characters_question', 'num_characters_answer'])\n",
        "\n",
        "# Calculating the average\n",
        "average_chars_context = df_dataset['num_characters_context'].mean()\n",
        "average_chars_question = df_dataset['num_characters_question'].mean()\n",
        "average_chars_answer = df_dataset['num_characters_answer'].mean()\n",
        "\n",
        "print(f'Average number of tokens in the context column: {(average_chars_context / 3):.0f}')\n",
        "print(f'Average number of tokens in the question column: {(average_chars_question / 3):.0f}')\n",
        "print(f'Average number of tokens in the answer column: {(average_chars_answer / 3):.0f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7WejJVkP-tF"
      },
      "outputs": [],
      "source": [
        "df_evaluation.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTpQH4u8tZGL"
      },
      "source": [
        "## Use base model to Inference\n",
        "\n",
        "1. The model is having shradded version of Mistral-7B which using 7 billion parameters distributed (dividing the parameters) into 8 different parts.\n",
        "\n",
        "2. This give us an advantage on efficient processing and training of very large models by distributing the computational load, especially when dealing with memory constraints on a single device in Google Colab Pro Subscription.\n",
        "\n",
        "3. with quantization parameters(tensors) using 16bit float representation requires 40GB A-100 NVIDIA GPU RAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcHaGW2vu9z7"
      },
      "source": [
        "### load base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QYkJ1CQu_us"
      },
      "outputs": [],
      "source": [
        "bnb_config_base_model: BitsAndBytesConfig = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzEa-J5pxQxj"
      },
      "outputs": [],
      "source": [
        "mistral_7b_sharded_base_model_name: str = \"alexsherstinsky/Mistral-7B-v0.1-sharded\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewkdH2XRvA0E"
      },
      "outputs": [],
      "source": [
        "base_model_tokenizer: LlamaTokenizerFast = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=mistral_7b_sharded_base_model_name, trust_remote_code=True, padding_side=\"left\")\n",
        "print(base_model_tokenizer.eos_token)\n",
        "base_model_tokenizer.pad_token = base_model_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5ijgbJTvCxE"
      },
      "outputs": [],
      "source": [
        "base_model: MistralForCausalLM = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=mistral_7b_sharded_base_model_name, device_map=\"auto\", torch_dtype=torch.float16, offload_folder=\"offload\", trust_remote_code=True, low_cpu_mem_usage=True, quantization_config=bnb_config_base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8D9_JzavJkm"
      },
      "source": [
        "### Inference on Base Model\n",
        "\n",
        "- The reason behind this step to understand how well the based model understands the context of the text present in the abstracts\n",
        "- This model helps in transfer learning process to the new model once it gets train on the new abstract data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeG6bcD-6LTQ"
      },
      "outputs": [],
      "source": [
        "df_inference_evaluation: pd.DataFrame = df_evaluation.head(10).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrcL0KQ96LTQ"
      },
      "outputs": [],
      "source": [
        "prompt_template_inference: str = \"\"\"\n",
        "[INST] <<SYS>>\n",
        "You are a helpful, detailed, and polite AI assistant.\n",
        "Answer the question using only the provided context.\n",
        "<</SYS>>\n",
        "\n",
        "### Input: {input}\n",
        "\n",
        "### Answer:\n",
        "[/INST]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6nxzJeB6LTR"
      },
      "outputs": [],
      "source": [
        "df_inference_evaluation[\"prompt\"] = df_inference_evaluation[\"input\"].apply(lambda x: prompt_template_inference.format(**{\"input\": x}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA4Cto596LTR"
      },
      "outputs": [],
      "source": [
        "base_model_sequences_generator: TextGenerationPipeline = transformers.pipeline(\n",
        "    task=\"text-generation\",\n",
        "    tokenizer=base_model_tokenizer,\n",
        "    model=base_model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_sequence = base_model_sequences_generator(\n",
        "    text_inputs=df_inference_evaluation[\"prompt\"].to_list(),\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=base_model_tokenizer.eos_token_id,\n",
        "    max_length=512,  # you can keep this if needed\n",
        "    max_new_tokens=100,  # Limit the number of tokens generated\n",
        "    truncation=True,  # Ensure truncation of long inputs\n",
        "    return_text=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LXIHdGjRWKlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnn--URZ6LTR"
      },
      "outputs": [],
      "source": [
        "print(f'\\n[BASE_MODEL_EVALUATION_BEGIN]')\n",
        "idx: int = 0\n",
        "print(f'\\n[=============EXAMPLE_{idx}_BEGIN=============]')\n",
        "for _, answer in zip(df_inference_evaluation[\"input\"].to_list(), base_model_sequence):\n",
        "  print(f'\\n[BASE_MODEL_EVALUATION] GENERATED_ANSWER:\\n{answer[0][\"generated_text\"]}')\n",
        "  print(f'\\n[=============EXAMPLE_{idx}_END=============]')\n",
        "  idx += 1\n",
        "\n",
        "print(f'\\n[BASE_MODEL_EVALUATION_END]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTmgCG7IaYbD"
      },
      "source": [
        "## finetuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAE3ryKQB74O"
      },
      "source": [
        "1. Temperature in generation: The lower the temperature parameter, the more conservative and deterministic the text generated by the model is, and it is more likely to select the word with the highest probability as the next word; while the higher the temperature parameter, the more diverse and more deterministic the text generated by the model is. It is possible to select words with lower probability or do more random sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV0H-bE_Ctwo"
      },
      "source": [
        "2. Adapter is used for fine tuning, which allows the model to learn additional knowledge on a specific task or data set while maintaining minor modifications to the overall structure of the model. Adapters can be added to individual layers of a pretrained model to allow fine-tuning or scaling without affecting the overall parameters of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFise1FliAuM"
      },
      "source": [
        "3. This part is: qlora_fine_tuning_config: dict = yaml.safe_load(qlora_fine_tuning_yaml).\n",
        "qlora_fine_tuning_yaml is a configuration file in YAML format, which contains configuration information related to migration learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj6IWPudkE-M"
      },
      "outputs": [],
      "source": [
        "qlora_fine_tuning_config: dict = yaml.safe_load(\n",
        "\"\"\"\n",
        "model_type: llm\n",
        "base_model: alexsherstinsky/Mistral-7B-v0.1-sharded\n",
        "\n",
        "input_features:\n",
        "  - name: prompt\n",
        "    type: text\n",
        "    preprocessing:\n",
        "      max_sequence_length: 256\n",
        "\n",
        "output_features:\n",
        "  - name: answer\n",
        "    type: text\n",
        "    preprocessing:\n",
        "      max_sequence_length: 256\n",
        "\n",
        "prompt:\n",
        "  template: |\n",
        "    [INST] <<SYS>>\n",
        "    You are a helpful, detailed, and polite AI assistant.\n",
        "    Answer the question using only the provided context.\n",
        "    <</SYS>>\n",
        "\n",
        "    ### Question: {question}\n",
        "    ### Context: {context}\n",
        "\n",
        "    ### Answer:\n",
        "    [/INST]\n",
        "\n",
        "generation:\n",
        "  temperature: 0.8\n",
        "  # max_new_tokens: 128\n",
        "  max_new_tokens: 150  # The max_token=177 of the data set answer is expected to be within this range.\n",
        "\n",
        "adapter:\n",
        "  type: lora\n",
        "  postprocessor:\n",
        "    merge_adapter_into_base_model: true\n",
        "    progressbar: true\n",
        "\n",
        "quantization:\n",
        "  bits: 8\n",
        "\n",
        "preprocessing:\n",
        "  global_max_sequence_length: 256\n",
        "  split:\n",
        "    # type: random\n",
        "    # probabilities: [0.7, 0.1, 0.2]  Originally 90% for training, 5% for validation, 5% for testing\n",
        "    type: fixed\n",
        "\n",
        "trainer:\n",
        "  type: finetune\n",
        "  train_steps: 50    # 3 individual epoch. train_steps * gradient_accumulation_steps * batch size = epoch * sample_train\n",
        "  epochs: 3\n",
        "  batch_size: 4\n",
        "  # steps_per_checkpoint: 500 # A total of 15 checkpoints are saved (originally 500)\n",
        "  checkpoints_per_epoch: 1\n",
        "  # eval_steps: 500\n",
        "  eval_batch_size: 8\n",
        "  early_stop: 3\n",
        "  gradient_accumulation_steps: 2  # effective batch size = batch size * gradient_accumulation_steps\n",
        "\n",
        "  learning_rate: 2.0e-4\n",
        "  enable_gradient_checkpointing: true\n",
        "  learning_rate_scheduler:\n",
        "    decay: cosine\n",
        "    warmup_fraction: 0.03\n",
        "    reduce_on_plateau: 0\n",
        "  use_mixed_precision: true\n",
        "  validation_field: combined\n",
        "  validation_metric: loss\n",
        "  enable_profiling: true  #Enable training process profiling using torch.profiler.profile\n",
        "  profiler:\n",
        "     wait: 1\n",
        "     warmup: 1\n",
        "     active: 3\n",
        "     repeat: 5\n",
        "     skip_first: 0\n",
        "  skip_all_evaluation: false\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZXGR8DJBqv"
      },
      "source": [
        "### Use LudwigModel for fine-tuning,\n",
        "\n",
        "LudwigModel is a library that is used to training models and using them to predict and evaluate them. It is based on datatype abstraction, so that the same data preprocessing and postprocessing will be performed on different datasets that share datatypes and the same encoding and decoding models developed can be re-used across several tasks.\n",
        "\n",
        "1. load the configuration file `qlora_fine_tuning_config` and build and train the model based on the parameters defined in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu2OVNUHHSyp"
      },
      "outputs": [],
      "source": [
        "model: LudwigModel = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz9cfrDeuJTs"
      },
      "source": [
        "Check GPU usage and clear CUDA before finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShhhdfQQt2HL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Get CUDA memory usage before running the code\n",
        "print(\"\\nBefore clearing CUDA cache:\")\n",
        "print(\"Current CUDA memory allocated: {:.2f} GB\".format(torch.cuda.memory_allocated() / 1024**3))\n",
        "print(\"Max CUDA memory allocated: {:.2f} GB\".format(torch.cuda.max_memory_allocated() / 1024**3))\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Get CUDA memory usage after running the code\n",
        "print(\"\\nAfter clearing CUDA cache:\")\n",
        "print(\"Current CUDA memory allocated: {:.2f} GB\".format(torch.cuda.memory_allocated() / 1024**3))\n",
        "print(\"Max CUDA memory allocated: {:.2f} GB\".format(torch.cuda.max_memory_allocated() / 1024**3))\n",
        "\n",
        "# Get the number of available GPUs\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(\"\\nNumber of available GPUs:\", num_gpus)\n",
        "\n",
        "# Iterate over each GPU and print its properties\n",
        "for i in range(num_gpus):\n",
        "    gpu_properties = torch.cuda.get_device_properties(i)\n",
        "    print(\"GPU {} - Total memory: {:.2f} GB\".format(i, gpu_properties.total_memory / 1024**3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnjgZF3kJpqt"
      },
      "source": [
        "The train method of the LudwigModel object is called to train the model using the given data set df_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABuMOeaYbGEO"
      },
      "outputs": [],
      "source": [
        "import gc # Replace with your actual variable names\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1GhYzDHtR-U"
      },
      "outputs": [],
      "source": [
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pZwlv5pTJiw"
      },
      "outputs": [],
      "source": [
        "results: TrainingResults = model.train(dataset=df_dataset,llm_int8_enable_fp32_cpu_offload=True, device_map=\"from_pretrained\")   # Will save relevant files in current path and create a ./results folder in current path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract 'Question' and 'Complex_CoT' from the dictionary inside 'train' column\n",
        "\n",
        "# If 'train' column is actually a dictionary, access its 'Question' key\n",
        "df_dataset['input'] = df_dataset.apply(\n",
        "    lambda row: f'Question: \"{row[\"train\"][\"Question\"]}\" \\n context: \"{row[\"train\"][\"Complex_CoT\"]}\"', axis=1\n",
        ")\n",
        "\n",
        "# Same for df_evaluation\n",
        "df_evaluation['input'] = df_evaluation.apply(\n",
        "    lambda row: f'Question: \"{row[\"train\"][\"Question\"]}\" \\n context: \"{row[\"train\"][\"Complex_CoT\"]}\"', axis=1\n",
        ")\n",
        "\n",
        "\n",
        "# Create the prompt template with placeholders\n",
        "prompt_template = \"Given the question: {input}, the expected answer is: {answer}\"\n",
        "\n",
        "# Pass the template to your training method\n",
        "results: TrainingResults = model.train(\n",
        "    dataset=df_dataset,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        "    device_map=\"from_pretrained\",\n",
        "    prompt_template=prompt_template  # Make sure you are passing the correct template\n",
        ")"
      ],
      "metadata": {
        "id": "c27QtJtiX106"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving model to Drive"
      ],
      "metadata": {
        "id": "85kdmEMZOqgq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-NQoDMHTC1p"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Define results saving path\n",
        "destination_path = drive_path+'./mistral-7b-ml'\n",
        "\n",
        "# Make sure the results save path exists\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# If the target path already exists, delete the contents in the target path first.\n",
        "if os.path.exists(destination_path):\n",
        "    shutil.rmtree(destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC1TuNXuTlff"
      },
      "outputs": [],
      "source": [
        "# Copy the results folder to the specified path\n",
        "shutil.copytree('./results', destination_path)  #Manually add the path created by the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YkjcqMFMkwT"
      },
      "source": [
        "### Perform Inferenceï¼ˆafter fine-tuningï¼‰\n",
        "\n",
        "We can now use the model we finetuned above to make predictions on some test examples to see whether finetuning the large language model improve its ability to follow instructions/the tasks we're asking it to perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spjEWbWIPUod"
      },
      "source": [
        "Use the trained Ludwig model to predict the evaluation data set df_evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqqSKijhkIRJ"
      },
      "source": [
        "Use the model_predict method to make predictions on the evaluation data set df_evaluation. The returned result is a tuple containing two DataFrames: predictions_and_probabilities. The first DataFrame contains the prediction results, and the second DataFrame contains the corresponding probability values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cBLw6eIF-5S"
      },
      "outputs": [],
      "source": [
        "df_evaluation_1 = df_evaluation.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sQXLxvoMuIq"
      },
      "outputs": [],
      "source": [
        "predictions_and_probabilities: tuple[pd.DataFrame, pd.DataFrame] = model.predict(df_evaluation_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPP1N-W0k0rn"
      },
      "source": [
        "Extract the DataFrame where the prediction results are located from the tuple predictions_and_probabilities and assign it to the variable df_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9Ri3EUzM6D4"
      },
      "outputs": [],
      "source": [
        "df_predictions: pd.DataFrame = predictions_and_probabilities[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaBlVTVRNElt"
      },
      "outputs": [],
      "source": [
        "df_predictions.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehabc-HB2NKL"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\n\")\n",
        "for prompt_with_summary in zip(df_evaluation_1['input'], df_predictions['answer_response']):\n",
        "  print(f\"Input:\\n{prompt_with_summary[0]}\")\n",
        "  print(f\"Generated Answer:\\n{prompt_with_summary[1][0]}\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ESNBtvmXoE"
      },
      "source": [
        "\n",
        "Evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzr-IolpWDuw"
      },
      "outputs": [],
      "source": [
        "!pip install rouge\n",
        "!pip install bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3rNBNhOTeB2"
      },
      "outputs": [],
      "source": [
        "answer = df_predictions['answer_response'].apply(lambda x: x[0])  # Generated answer\n",
        "ground_truth = df_evaluation.head(20)['answer']  # Refer to answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "!pip install bert-score\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "1pbbYBh7MPN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KKqGMBlWQZG"
      },
      "outputs": [],
      "source": [
        "# Optimized code\n",
        "# semantic similarity (METEOR and BERTScore)\n",
        "# word and phrase level overlap (BLEU and ROUGE scores)\n",
        "\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge import Rouge\n",
        "from bert_score import score\n",
        "\n",
        "\n",
        "\n",
        "def calculate_scores(base_answers, predict_answers):\n",
        "    total_bert_score = 0\n",
        "    total_meteor_score = 0\n",
        "    total_bleu_score = 0\n",
        "    total_rouge_score = {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0}\n",
        "    num_results = len(predict_answers)\n",
        "\n",
        "    for answer, ground_truth in zip(predict_answers, base_answers):\n",
        "        if not answer or not ground_truth:\n",
        "            continue  # Skip empty answers or ground truths\n",
        "\n",
        "        # Tokenize hypothesis and reference\n",
        "        hypothesis_tokens = word_tokenize(answer)\n",
        "        reference_tokens = word_tokenize(ground_truth)\n",
        "\n",
        "        # BERTScore\n",
        "        _, _, F1 = score([answer], [ground_truth], lang='en', verbose=False)\n",
        "        total_bert_score += F1.item()\n",
        "\n",
        "        # METEOR\n",
        "        meteor = meteor_score([reference_tokens], hypothesis_tokens)\n",
        "        total_meteor_score += meteor\n",
        "\n",
        "        # BLEU\n",
        "        bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None)\n",
        "        total_bleu_score += bleu_score\n",
        "\n",
        "        # ROUGE\n",
        "        rouge = Rouge()\n",
        "        rouge_scores = rouge.get_scores(answer, ground_truth)[0]\n",
        "        for metric, scores in rouge_scores.items():\n",
        "            total_rouge_score[metric] += scores['f']\n",
        "\n",
        "    average_bert_score = total_bert_score / num_results\n",
        "    average_meteor_score = total_meteor_score / num_results\n",
        "    average_bleu_score = total_bleu_score / num_results\n",
        "    average_rouge_score = {metric: score / num_results for metric, score in total_rouge_score.items()}\n",
        "\n",
        "    return average_bert_score, average_meteor_score, average_bleu_score, average_rouge_score\n",
        "\n",
        "\n",
        "def load_results(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "        return results\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found:\", file_path)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Invalid JSON format in file:\", file_path)\n",
        "    return []\n",
        "\n",
        "\n",
        "average_bert_score, average_meteor_score, average_bleu_score, average_rouge_score = calculate_scores(ground_truth, answer)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average BERTScore:\", average_bert_score)\n",
        "print(\"Average METEOR score:\", average_meteor_score)\n",
        "print(\"Average BLEU score:\", average_bleu_score)\n",
        "print(\"Average Rouge score:\", average_rouge_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze and evaluate this result\n",
        "\n",
        "\n",
        "These results are an evaluation of the model performance. The following is an analysis and evaluation for each indicator:\n",
        "\n",
        "Average BERTScore (0.84):\n",
        "BERTScore is a metric used to measure the semantic similarity between the generated text and the reference text. It uses the pre-trained BERT model to encode the sentences and calculate the similarity score between them. The average BERTScore here is 0.8682, indicating that the semantic similarity between the text generated by the model and the reference text is high.\n",
        "\n",
        "Average METEOR Score (0.32):\n",
        "The METEOR score is another metric for evaluating the quality of machine translation. It takes into account word-level alignment as well as sentence-level semantic similarity. The average METEOR score is 0.3815, which is relatively high, indicating that the text generated by the model is consistent with the reference text to a certain extent.\n",
        "\n",
        "Average BLEU Score (0.0868):\n",
        "The BLEU score is used to evaluate the quality of machine translation, and its range is usually between 0 and 1, where 1 indicates a perfect match. The average BLEU score here is about 0.1394, which means that the match between the text generated by the model and the reference text is relatively low. Possible reasons include differences in vocabulary selection, syntactic structure, etc.\n",
        "\n",
        "Average Rouge Score:\n",
        "ROUGE scores are used to evaluate the degree of overlap between the generated text and the reference text, including word-level and sentence-level overlap. The average scores of the three ROUGE indicators are provided here:\n",
        "\n",
        "rouge-1: The average value is about 0.321, indicating that the overlap between the single words generated by the model and the single words in the reference text is good.\n",
        "\n",
        "rouge-2: The average value is about 0.124, indicating that the overlap between the phrases composed of two words generated by the model and the phrases in the reference text is low.\n",
        "\n",
        "rouge-l: The average value is about 0.295, indicating that the length of the longest common subsequence between the text generated by the model and the reference text is high, that is, the overlap at the sentence level is good.\n",
        "\n",
        "Overall, the model performs well in terms of semantic similarity (high METEOR and BERTScore), but there may be room for improvement in terms of word and phrase-level overlap (relatively low BLEU and ROUGE scores). Possible improvements include model tuning, better training data, improved generation strategies, etc."
      ],
      "metadata": {
        "id": "Ac4gGixVSlC5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcrji3tPwe2Y"
      },
      "source": [
        "## Use model for question answering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template: str = \"\"\"\n",
        "You are a helpful, respectful and honest assistant. \\\n",
        "Your task is to generate an answer to the given question. \\\n",
        "And your answer should be based on the provided context only.\n",
        "\n",
        "### input: {prompt}\n",
        "\n",
        "### Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nhOB6n-gRxbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/project-kalki/mistral-7b-ml/api_experiment_run/model/model_weights'\n",
        "tokenizer: LlamaTokenizerFast = AutoTokenizer.from_pretrained(\n",
        "  pretrained_model_name_or_path = model_path,\n",
        "  trust_remote_code=True,\n",
        "  padding_side=\"left\"\n",
        ")\n",
        "\n",
        "bnb_config_samsum_fine_tuned_model: BitsAndBytesConfig = BitsAndBytesConfig()\n",
        "\n",
        "model_load: MistralForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_path,\n",
        "    # torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config_samsum_fine_tuned_model,\n",
        "    # low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "generator: TextGenerationPipeline = transformers.pipeline(\n",
        "    task=\"text-generation\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model_load,\n",
        "    # torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "3mG52RckpaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(user_input):\n",
        "  prompt = prompt_template.format(prompt=user_input)\n",
        "  print(prompt)\n",
        "  return generator(user_input)[0]['generated_text']\n",
        "\n",
        "while True:\n",
        "  user_input = input('Please enter question for an article: ')\n",
        "\n",
        "  if user_input == 'exit':\n",
        "    break\n",
        "\n",
        "  print(infer(user_input))"
      ],
      "metadata": {
        "id": "e8ww8jyw-Qbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
